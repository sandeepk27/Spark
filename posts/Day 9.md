---
title: 'Day 9: Spark SQL Deep Dive - Temp Views, Query Execution & Optimization Tips for Data Engineers'
published: true
description: 'Learn how to optimize Spark Joins using broadcast variables, skew handling, and strategic repartitioning.'
tags: 'python, dataengineering, spark, bigdata'
linkedin_image: 'no'
devto_cover: 'no'
---

Welcome to Day 9 of the Spark Mastery Series!
Today we explore one of the most widely used features in PySpark â€” Spark SQL.

Even if you're a Python engineer, SQL is STILL the easiest and fastest way to express business logic. Spark SQL allows you to use familiar SQL syntax while leveraging Sparkâ€™s distributed execution engine.

Letâ€™s dive in.

ğŸŒŸ **1. What is Spark SQL?**

Spark SQL enables:
- Writing SQL directly on DataFrames
- Querying structured + semi-structured data
- Using SQL to express ETL transformations
- Creating tables and views
- Integrating with BI tools

It is powered by Catalyst Optimizer, making SQL queries in Spark both expressive and fast.

ğŸŒŸ **2. Creating Temporary Views â€” The Bridge Between SQL and DataFrames**

ğŸŸ¢ Local Temp View

Visible only inside current session:

```
df.createOrReplaceTempView("employees")
spark.sql("SELECT * FROM employees")
```

ğŸ”µ Global Temp View

Visible across sessions and Spark applications:

```
df.createOrReplaceGlobalTempView("employees")
spark.sql("SELECT * FROM global_temp.employees")
```

Useful in:
- Multi-session workloads
- Shared ETL pipelines


ğŸŒŸ **3. Running SQL Queries**

```
spark.sql("""
SELECT dept, COUNT(*) AS total 
FROM employees 
GROUP BY dept
""").show()
```

You can do everything SQL supports:
- joins
- aggregations
- subqueries
- window functions
- CTEs

ğŸŒŸ **4. Spark Catalog â€” Explore Your Data Environment**

Spark maintains metadata of:

- tables
- functions
- databases

Useful commands:

```
spark.catalog.listTables()
spark.catalog.listDatabases()
spark.catalog.listColumns("employees")
```
This helps engineers understand schema dependencies in pipelines.

ğŸŒŸ **5. SQL Table Creation**

Spark SQL supports DDL statements:
```
CREATE TABLE sales 
USING parquet 
OPTIONS (path '/mnt/data/sales');
```

To insert:
```
INSERT INTO sales SELECT * FROM new_sales;
```

ğŸŒŸ **6. Query Execution Explained**

Steps: 
1ï¸âƒ£ Parse SQL query
2ï¸âƒ£ Build Logical Plan
3ï¸âƒ£ Optimize plan using Catalyst
4ï¸âƒ£ Build Physical Plan
5ï¸âƒ£ Execute using Tungsten engine

```
spark.sql("SELECT * FROM employees").explain(True)
```

This prints:
- Parsed logical plan
- Analyzed logical plan
- Optimized logical plan
- Physical plan

ğŸŒŸ **7. SQL Performance Optimization Tips**
âœ” Always filter on partition columns
âœ” Use Parquet OR Delta instead of CSV
âœ” Avoid UDFs â€” they break optimization
âœ” Use broadcast on small tables
âœ” Avoid unnecessary ORDER BY (shuffle-heavy)
âœ” Avoid SELECT *

ğŸŒŸ **8. Register UDFs in SQL**

Sometimes SQL needs Python logic.

```
spark.udf.register("upper_udf", lambda x: x.upper(), "string")

spark.sql("SELECT upper_udf(name) FROM employees")
```

But remember:

âš  UDFs disable many Catalyst optimizations â†’ try to avoid unless necessary.


ğŸš€ **Summary**
Today we learned:
- Temp Views
- Global Temp Views
- Spark SQL
- Catalog functions
- SQL optimization
- Query execution plan
- SQL-UDF integration

Follow for more such content. Let me know if I missed anything in comments. Thank you!!
